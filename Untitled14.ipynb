{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd7188-ba70-400e-9a76-67f19a2802e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 14:17:17,713 - INFO - Starting Enhanced Document GAN\n",
      "2025-10-18 14:17:17,714 - INFO - Resolution: 512x512\n",
      "2025-10-18 14:17:17,714 - INFO - Device: cpu\n",
      "2025-10-18 14:17:17,717 - INFO - Creating dataset...\n",
      "2025-10-18 14:17:17,718 - INFO - Found 5 fonts\n",
      "2025-10-18 14:17:17,718 - INFO - Creating readable document samples...\n",
      "2025-10-18 14:17:57,475 - INFO - Completed creating readable documents\n",
      "2025-10-18 14:17:57,480 - INFO - Dataset loaded with 449 images\n",
      "2025-10-18 14:17:57,480 - INFO - Dataset: 449 images, Batches: 56\n",
      "2025-10-18 14:17:57,551 - INFO - Batch shape: torch.Size([8, 1, 512, 512])\n",
      "2025-10-18 14:17:57,551 - INFO - Value range: [-1.000, 0.984]\n",
      "2025-10-18 14:17:57,869 - INFO - Generator parameters: 23,328,363\n",
      "2025-10-18 14:17:57,869 - INFO - Discriminator parameters: 11,243,904\n",
      "2025-10-18 14:17:57,869 - INFO - Trainer initialized on cpu\n",
      "2025-10-18 14:17:57,878 - INFO - Trainer initialized successfully\n",
      "2025-10-18 14:17:57,878 - INFO - Starting training...\n",
      "2025-10-18 14:17:57,878 - INFO - Starting training...\n",
      "2025-10-18 14:18:04,749 - INFO - Epoch [  1/700] Batch [  0/56] D_Loss: 125.6258 G_Loss: 219.8934 SSIM: 0.278 PSNR: 6.76\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9852\\2990567954.py:847: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  Image.fromarray(img_array, mode='L').save(\n",
      "2025-10-18 14:18:06,928 - INFO - Saved progress for epoch 1\n",
      "2025-10-18 14:19:06,370 - INFO - Epoch [  1/700] Batch [ 10/56] D_Loss: 28.9426 G_Loss: 208.5943 SSIM: 0.294 PSNR: 10.33\n",
      "2025-10-18 14:20:06,211 - INFO - Epoch [  1/700] Batch [ 20/56] D_Loss: 28.6463 G_Loss: 214.4842 SSIM: 0.345 PSNR: 8.98\n",
      "2025-10-18 14:21:06,199 - INFO - Epoch [  1/700] Batch [ 30/56] D_Loss: 36.8103 G_Loss: 224.8361 SSIM: 0.389 PSNR: 7.68\n",
      "2025-10-18 14:24:31,531 - INFO - Epoch [  1/700] Batch [ 40/56] D_Loss: 29.3227 G_Loss: 214.4716 SSIM: 0.619 PSNR: 8.26\n",
      "2025-10-18 14:27:35,083 - INFO - Epoch [  1/700] Batch [ 50/56] D_Loss: 29.1662 G_Loss: 194.2769 SSIM: 0.689 PSNR: 8.88\n",
      "2025-10-18 14:27:37,180 - INFO - Saved progress for epoch 1\n",
      "2025-10-18 14:28:06,961 - INFO - Epoch [  1/700] completed in 609.08s - D_Loss: 39.7656 G_Loss: 204.3240 SSIM: 0.449 PSNR: 9.16\n",
      "2025-10-18 14:28:07,536 - INFO - New best SSIM: 0.4487 - Model saved!\n",
      "2025-10-18 14:28:13,487 - INFO - Epoch [  2/700] Batch [  0/56] D_Loss: 30.0000 G_Loss: 180.7194 SSIM: 0.729 PSNR: 9.22\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import platform\n",
    "\n",
    "# Platform-specific configuration for Windows compatibility\n",
    "if platform.system() == 'Windows':\n",
    "    try:\n",
    "        torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "    DATALOADER_NUM_WORKERS = 0\n",
    "else:\n",
    "    DATALOADER_NUM_WORKERS = 2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class Config:\n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 2\n",
    "    LEARNING_RATE_G = 0.0002\n",
    "    LEARNING_RATE_D = 0.0001\n",
    "    EPOCHS = 700\n",
    "    \n",
    "    # Image parameters\n",
    "    IMG_HEIGHT = 512\n",
    "    IMG_WIDTH = 512\n",
    "    CHANNELS = 1\n",
    "    \n",
    "    # GAN parameters\n",
    "    LATENT_DIM = 128\n",
    "    FEATURE_DIM = 64\n",
    "    \n",
    "    # Loss weights\n",
    "    LAMBDA_ADVERSARIAL = 1.0\n",
    "    LAMBDA_L1 = 100.0\n",
    "    LAMBDA_PERCEPTUAL = 20.0\n",
    "    LAMBDA_SSIM = 10.0\n",
    "    LAMBDA_EDGE = 15.0\n",
    "    LAMBDA_TV = 2.0\n",
    "    LAMBDA_TEXT_STRUCTURE = 30.0\n",
    "    \n",
    "    # Quality targets\n",
    "    TARGET_SSIM = 0.85\n",
    "    TARGET_PSNR = 30.0\n",
    "    \n",
    "    # Paths\n",
    "    REAL_IMAGES_DIR = \"Dataset/Ancient palm leaf documents/Jathakam/Original/dummy_sample\"\n",
    "    OUTPUT_DIR = \"Enhanced_GAN_Output2\"\n",
    "    MODELS_DIR = \"Enhanced_GAN_Models2\"\n",
    "    GENERATED_IMAGES_DIR = \"Generated_Images3\"\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Training stability parameters\n",
    "    GRADIENT_PENALTY_LAMBDA = 10.0\n",
    "    SPECTRAL_NORM = False\n",
    "    \n",
    "    # Document generation parameters\n",
    "    MIN_FONT_SIZE = 14\n",
    "    MAX_FONT_SIZE = 20\n",
    "    LINE_SPACING = 25\n",
    "    PARAGRAPH_SPACING = 35\n",
    "    CHAR_DETAIL_LEVEL = 5\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ==================== ENHANCED LOSS FUNCTIONS ====================\n",
    "class TextStructureLoss(nn.Module):\n",
    "    \"\"\"Loss function for text structure preservation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextStructureLoss, self).__init__()\n",
    "        \n",
    "        # Fixed: Proper 4D tensor shape [out_channels, in_channels, height, width]\n",
    "        horizontal_kernel = torch.tensor([[[[-1, -1, -1],\n",
    "                                            [ 2,  2,  2],\n",
    "                                            [-1, -1, -1]]]], dtype=torch.float32)\n",
    "        \n",
    "        vertical_kernel = torch.tensor([[[[-1, 2, -1],\n",
    "                                          [-1, 2, -1],\n",
    "                                          [-1, 2, -1]]]], dtype=torch.float32)\n",
    "        \n",
    "        self.register_buffer('h_kernel', horizontal_kernel)\n",
    "        self.register_buffer('v_kernel', vertical_kernel)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred_h = F.conv2d(pred, self.h_kernel, padding=1)\n",
    "        target_h = F.conv2d(target, self.h_kernel, padding=1)\n",
    "        \n",
    "        pred_v = F.conv2d(pred, self.v_kernel, padding=1)\n",
    "        target_v = F.conv2d(target, self.v_kernel, padding=1)\n",
    "        \n",
    "        h_loss = F.l1_loss(pred_h, target_h)\n",
    "        v_loss = F.l1_loss(pred_v, target_v)\n",
    "        \n",
    "        return h_loss + v_loss\n",
    "\n",
    "class EnhancedPerceptualLoss(nn.Module):\n",
    "    \"\"\"Enhanced perceptual loss for text features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EnhancedPerceptualLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        scales = [1, 2, 4]\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for i, scale in enumerate(scales):\n",
    "            if scale > 1:\n",
    "                pred_scaled = F.avg_pool2d(pred, scale)\n",
    "                target_scaled = F.avg_pool2d(target, scale)\n",
    "            else:\n",
    "                pred_scaled = pred\n",
    "                target_scaled = target\n",
    "            \n",
    "            weight = 1.0 / (scale * 0.5)\n",
    "            total_loss += weight * F.mse_loss(pred_scaled, target_scaled)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "class EnhancedEdgeLoss(nn.Module):\n",
    "    \"\"\"Enhanced edge loss for character boundaries.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EnhancedEdgeLoss, self).__init__()\n",
    "        \n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
    "        laplacian = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=torch.float32)\n",
    "        \n",
    "        self.register_buffer('sobel_x', sobel_x.view(1, 1, 3, 3))\n",
    "        self.register_buffer('sobel_y', sobel_y.view(1, 1, 3, 3))\n",
    "        self.register_buffer('laplacian', laplacian.view(1, 1, 3, 3))\n",
    "    \n",
    "    def get_edges(self, x):\n",
    "        edge_x = F.conv2d(x, self.sobel_x, padding=1)\n",
    "        edge_y = F.conv2d(x, self.sobel_y, padding=1)\n",
    "        edge_lap = F.conv2d(x, self.laplacian, padding=1)\n",
    "        \n",
    "        sobel_edges = torch.sqrt(edge_x**2 + edge_y**2 + 1e-8)\n",
    "        return sobel_edges + 0.5 * torch.abs(edge_lap)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred_edges = self.get_edges(pred)\n",
    "        target_edges = self.get_edges(target)\n",
    "        return F.l1_loss(pred_edges, target_edges)\n",
    "\n",
    "class SimpleSSIMLoss(nn.Module):\n",
    "    \"\"\"Simple SSIM loss for structural similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SimpleSSIMLoss, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        \n",
    "        sigma = 1.5\n",
    "        gauss = torch.Tensor([math.exp(-(x - window_size//2)**2 / float(2*sigma**2)) \n",
    "                             for x in range(window_size)])\n",
    "        gauss = gauss / gauss.sum()\n",
    "        \n",
    "        window = gauss.unsqueeze(1) @ gauss.unsqueeze(0)\n",
    "        window = window.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('window', window)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "        \n",
    "        mu1 = F.conv2d(pred, self.window, padding=self.window_size//2, groups=self.channel)\n",
    "        mu2 = F.conv2d(target, self.window, padding=self.window_size//2, groups=self.channel)\n",
    "        \n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        sigma1_sq = F.conv2d(pred * pred, self.window, padding=self.window_size//2, groups=self.channel) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(target * target, self.window, padding=self.window_size//2, groups=self.channel) - mu2_sq\n",
    "        sigma12 = F.conv2d(pred * target, self.window, padding=self.window_size//2, groups=self.channel) - mu1_mu2\n",
    "        \n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "                   ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "        \n",
    "        if self.size_average:\n",
    "            return 1 - ssim_map.mean()\n",
    "        else:\n",
    "            return 1 - ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "# ==================== GENERATOR ARCHITECTURE ====================\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.size()\n",
    "        \n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, H * W)\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, H * W)\n",
    "        \n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(attention)\n",
    "        \n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, H, W)\n",
    "        \n",
    "        return self.gamma * out + x\n",
    "\n",
    "class EnhancedGenerator(nn.Module):\n",
    "    \"\"\"Enhanced generator with attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EnhancedGenerator, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config.LATENT_DIM, 1024 * 8 * 8),\n",
    "            nn.BatchNorm1d(1024 * 8 * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_layer(1024, 512, use_attention=True)\n",
    "        self.layer2 = self._make_layer(512, 256, use_attention=True)\n",
    "        self.layer3 = self._make_layer(256, 128, use_attention=False)\n",
    "        self.layer4 = self._make_layer(128, 64, use_attention=False)\n",
    "        self.layer5 = self._make_layer(64, 32, use_attention=False)\n",
    "        self.layer6 = self._make_layer(32, 16, use_attention=False)\n",
    "        \n",
    "        self.text_refine = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, 3, 1, 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 4, 3, 1, 1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(4, config.CHANNELS, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, use_attention=False):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if use_attention:\n",
    "            layers.append(SelfAttention(out_channels))\n",
    "        \n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        x = self.fc(z)\n",
    "        x = x.view(batch_size, 1024, 8, 8)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        \n",
    "        x = self.text_refine(x)\n",
    "        \n",
    "        return self.final(x)\n",
    "\n",
    "class EnhancedDiscriminator(nn.Module):\n",
    "    \"\"\"Enhanced discriminator.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EnhancedDiscriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(config.CHANNELS, 32, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(1024, 1, 8, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1).squeeze(1)\n",
    "\n",
    "# ==================== DATASET ====================\n",
    "class ReadableDocumentDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with readable text.\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir, create_samples=True):\n",
    "        self.images_dir = os.path.abspath(images_dir)\n",
    "        self.fonts = self._get_available_fonts()\n",
    "        \n",
    "        if create_samples:\n",
    "            self._create_readable_documents()\n",
    "        \n",
    "        self.image_paths = self._get_image_paths()\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            self._create_font_fallback()\n",
    "            self.image_paths = self._get_image_paths()\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((config.IMG_HEIGHT, config.IMG_WIDTH)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "        \n",
    "        logger.info(f\"Dataset loaded with {len(self.image_paths)} images\")\n",
    "    \n",
    "    def _get_available_fonts(self):\n",
    "        \"\"\"Get available fonts.\"\"\"\n",
    "        fonts = []\n",
    "        \n",
    "        font_paths = [\n",
    "            \"C:/Windows/Fonts/arial.ttf\",\n",
    "            \"C:/Windows/Fonts/times.ttf\",\n",
    "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "            \"/System/Library/Fonts/Arial.ttf\",\n",
    "        ]\n",
    "        \n",
    "        for font_path in font_paths:\n",
    "            if os.path.exists(font_path):\n",
    "                try:\n",
    "                    for size in [12, 14, 16, 18, 20]:\n",
    "                        fonts.append(ImageFont.truetype(font_path, size))\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if not fonts:\n",
    "            try:\n",
    "                for size in [12, 14, 16, 18, 20]:\n",
    "                    fonts.append(ImageFont.load_default())\n",
    "            except:\n",
    "                fonts = [None] * 5\n",
    "        \n",
    "        logger.info(f\"Found {len([f for f in fonts if f is not None])} fonts\")\n",
    "        return fonts\n",
    "    \n",
    "    def _get_image_paths(self):\n",
    "        if not os.path.exists(self.images_dir):\n",
    "            return []\n",
    "        \n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "        paths = []\n",
    "        \n",
    "        for file in os.listdir(self.images_dir):\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                paths.append(os.path.join(self.images_dir, file))\n",
    "        \n",
    "        return sorted(paths)\n",
    "    \n",
    "    def _create_readable_documents(self):\n",
    "        \"\"\"Create readable documents.\"\"\"\n",
    "        logger.info(\"Creating readable document samples...\")\n",
    "        os.makedirs(self.images_dir, exist_ok=True)\n",
    "        \n",
    "        document_contents = [\n",
    "            {\n",
    "                \"title\": \"ARTIFICIAL INTELLIGENCE IN MODERN COMPUTING\",\n",
    "                \"sections\": [\n",
    "                    {\n",
    "                        \"heading\": \"1. Introduction\",\n",
    "                        \"content\": [\n",
    "                            \"Artificial Intelligence has revolutionized the way we approach complex problems\",\n",
    "                            \"in computer science. Machine learning algorithms have become increasingly\",\n",
    "                            \"sophisticated, enabling computers to learn from data and make predictions\",\n",
    "                            \"with remarkable accuracy. Deep learning, a subset of machine learning,\",\n",
    "                            \"has shown particular promise in areas such as image recognition, natural\",\n",
    "                            \"language processing, and autonomous systems.\"\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"heading\": \"2. Methodology\",\n",
    "                        \"content\": [\n",
    "                            \"Our research employs a comprehensive approach to evaluate AI performance\",\n",
    "                            \"across multiple domains. We collected datasets from various sources and\",\n",
    "                            \"implemented state-of-the-art neural network architectures. The training\",\n",
    "                            \"process involved careful hyperparameter tuning and cross-validation to\",\n",
    "                            \"ensure robust results. Performance metrics included accuracy, precision,\",\n",
    "                            \"recall, and F1-score for classification tasks.\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"DATA SCIENCE AND STATISTICAL ANALYSIS\",\n",
    "                \"sections\": [\n",
    "                    {\n",
    "                        \"heading\": \"Abstract\",\n",
    "                        \"content\": [\n",
    "                            \"This paper presents a comprehensive analysis of statistical methods used\",\n",
    "                            \"in modern data science applications. We examine the effectiveness of\",\n",
    "                            \"various techniques including regression analysis, hypothesis testing,\",\n",
    "                            \"and Bayesian inference. The results demonstrate significant improvements\",\n",
    "                            \"in prediction accuracy when proper statistical foundations are applied\",\n",
    "                            \"to machine learning models.\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i in range(200):\n",
    "            img = Image.new('RGB', (config.IMG_WIDTH, config.IMG_HEIGHT), color=(250, 250, 250))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            doc_content = random.choice(document_contents)\n",
    "            \n",
    "            left_margin = 40\n",
    "            right_margin = config.IMG_WIDTH - 40\n",
    "            top_margin = 50\n",
    "            y_pos = top_margin\n",
    "            \n",
    "            # Draw title\n",
    "            title_font = self._get_font(18)\n",
    "            if title_font:\n",
    "                try:\n",
    "                    title_bbox = draw.textbbox((0, 0), doc_content[\"title\"], font=title_font)\n",
    "                    title_width = title_bbox[2] - title_bbox[0]\n",
    "                except:\n",
    "                    title_width = len(doc_content[\"title\"]) * 10\n",
    "                \n",
    "                title_x = (config.IMG_WIDTH - title_width) // 2\n",
    "                draw.text((title_x, y_pos), doc_content[\"title\"], fill=(0, 0, 0), font=title_font)\n",
    "                y_pos += 50\n",
    "                \n",
    "                draw.line([(left_margin, y_pos), (right_margin, y_pos)], fill=(0, 0, 0), width=2)\n",
    "                y_pos += 30\n",
    "            \n",
    "            # Draw sections\n",
    "            for section in doc_content[\"sections\"]:\n",
    "                if y_pos > config.IMG_HEIGHT - 100:\n",
    "                    break\n",
    "                \n",
    "                heading_font = self._get_font(16)\n",
    "                if heading_font:\n",
    "                    draw.text((left_margin, y_pos), section[\"heading\"], fill=(0, 0, 0), font=heading_font)\n",
    "                    y_pos += 35\n",
    "                \n",
    "                content_font = self._get_font(14)\n",
    "                if content_font:\n",
    "                    for line in section[\"content\"]:\n",
    "                        words = line.split()\n",
    "                        current_line = \"\"\n",
    "                        \n",
    "                        for word in words:\n",
    "                            test_line = current_line + (\" \" if current_line else \"\") + word\n",
    "                            \n",
    "                            try:\n",
    "                                bbox = draw.textbbox((0, 0), test_line, font=content_font)\n",
    "                                line_width = bbox[2] - bbox[0]\n",
    "                            except:\n",
    "                                line_width = len(test_line) * 8\n",
    "                            \n",
    "                            if line_width > (right_margin - left_margin):\n",
    "                                if current_line:\n",
    "                                    draw.text((left_margin, y_pos), current_line, \n",
    "                                            fill=(0, 0, 0), font=content_font)\n",
    "                                    y_pos += config.LINE_SPACING\n",
    "                                    current_line = word\n",
    "                            else:\n",
    "                                current_line = test_line\n",
    "                        \n",
    "                        if current_line:\n",
    "                            draw.text((left_margin, y_pos), current_line, \n",
    "                                    fill=(0, 0, 0), font=content_font)\n",
    "                            y_pos += config.LINE_SPACING\n",
    "                    \n",
    "                    y_pos += 20\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            img_gray = img.convert('L')\n",
    "            \n",
    "            # Add slight noise\n",
    "            img_array = np.array(img_gray)\n",
    "            noise = np.random.normal(0, 0.5, img_array.shape)\n",
    "            img_array = np.clip(img_array.astype(float) + noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Fixed: Remove deprecated mode parameter\n",
    "            final_img = Image.fromarray(img_array)\n",
    "            final_img.save(os.path.join(self.images_dir, f'readable_doc_{i:04d}.png'))\n",
    "        \n",
    "        logger.info(\"Completed creating readable documents\")\n",
    "    \n",
    "    def _get_font(self, size):\n",
    "        \"\"\"Get font for size.\"\"\"\n",
    "        if not self.fonts or all(f is None for f in self.fonts):\n",
    "            return None\n",
    "        \n",
    "        size_map = {12: 0, 14: 1, 16: 2, 18: 3, 20: 4}\n",
    "        font_index = size_map.get(size, 1)\n",
    "        \n",
    "        if font_index < len(self.fonts):\n",
    "            return self.fonts[font_index]\n",
    "        return self.fonts[0] if self.fonts[0] is not None else None\n",
    "    \n",
    "    def _create_font_fallback(self):\n",
    "        \"\"\"Create fallback documents.\"\"\"\n",
    "        logger.info(\"Creating fallback documents...\")\n",
    "        os.makedirs(self.images_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(100):\n",
    "            img = np.ones((config.IMG_HEIGHT, config.IMG_WIDTH), dtype=np.uint8) * 248\n",
    "            \n",
    "            y = 80\n",
    "            while y < config.IMG_HEIGHT - 80:\n",
    "                if random.random() > 0.1:\n",
    "                    x = 60\n",
    "                    while x < config.IMG_WIDTH - 100:\n",
    "                        word_length = random.randint(30, 120)\n",
    "                        char_height = random.randint(14, 18)\n",
    "                        \n",
    "                        img[y:y+char_height, x:x+word_length] = random.randint(20, 40)\n",
    "                        \n",
    "                        x += word_length + random.randint(10, 20)\n",
    "                        \n",
    "                        if x >= config.IMG_WIDTH - 150:\n",
    "                            break\n",
    "                \n",
    "                y += random.randint(20, 28)\n",
    "                \n",
    "                if random.random() > 0.85:\n",
    "                    y += random.randint(20, 35)\n",
    "            \n",
    "            Image.fromarray(img, mode='L').save(\n",
    "                os.path.join(self.images_dir, f'fallback_doc_{i:04d}.png')\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx % len(self.image_paths)]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as image:\n",
    "                image = image.convert('L')\n",
    "                return self.transform(image)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading {img_path}: {e}\")\n",
    "            fallback = np.ones((config.IMG_HEIGHT, config.IMG_WIDTH), dtype=np.uint8) * 245\n",
    "            fallback_pil = Image.fromarray(fallback, mode='L')\n",
    "            return self.transform(fallback_pil)\n",
    "\n",
    "# ==================== TRAINER ====================\n",
    "class EnhancedGANTrainer:\n",
    "    \"\"\"Enhanced GAN trainer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.generator = EnhancedGenerator().to(config.DEVICE)\n",
    "        self.discriminator = EnhancedDiscriminator().to(config.DEVICE)\n",
    "        \n",
    "        g_params = sum(p.numel() for p in self.generator.parameters() if p.requires_grad)\n",
    "        d_params = sum(p.numel() for p in self.discriminator.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Generator parameters: {g_params:,}\")\n",
    "        logger.info(f\"Discriminator parameters: {d_params:,}\")\n",
    "        \n",
    "        self.optimizer_G = optim.Adam(\n",
    "            self.generator.parameters(), \n",
    "            lr=config.LEARNING_RATE_G, \n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.optimizer_D = optim.Adam(\n",
    "            self.discriminator.parameters(), \n",
    "            lr=config.LEARNING_RATE_D, \n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        self.adversarial_loss = nn.BCELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.l2_loss = nn.MSELoss()\n",
    "        self.perceptual_loss = EnhancedPerceptualLoss().to(config.DEVICE)\n",
    "        self.ssim_loss = SimpleSSIMLoss().to(config.DEVICE)\n",
    "        self.edge_loss = EnhancedEdgeLoss().to(config.DEVICE)\n",
    "        self.text_structure_loss = TextStructureLoss().to(config.DEVICE)\n",
    "        \n",
    "        self.history = {\n",
    "            'g_loss': [], 'd_loss': [], 'ssim_scores': [], 'psnr_scores': [],\n",
    "            'g_adv_loss': [], 'g_content_loss': []\n",
    "        }\n",
    "        \n",
    "        # Fixed: Removed verbose parameter\n",
    "        self.scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer_G, mode='min', factor=0.5, patience=50\n",
    "        )\n",
    "        self.scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer_D, mode='min', factor=0.5, patience=50\n",
    "        )\n",
    "        \n",
    "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "        os.makedirs(config.MODELS_DIR, exist_ok=True)\n",
    "        os.makedirs(config.GENERATED_IMAGES_DIR, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Trainer initialized on {config.DEVICE}\")\n",
    "    \n",
    "    def train_discriminator(self, real_images):\n",
    "        \"\"\"Train discriminator.\"\"\"\n",
    "        self.optimizer_D.zero_grad()\n",
    "        \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, device=config.DEVICE) * 0.9\n",
    "        fake_labels = torch.zeros(batch_size, device=config.DEVICE) * 0.1\n",
    "        \n",
    "        real_output = self.discriminator(real_images)\n",
    "        real_loss = self.adversarial_loss(real_output, real_labels)\n",
    "        \n",
    "        noise = torch.randn(batch_size, config.LATENT_DIM, device=config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            fake_images = self.generator(noise)\n",
    "        \n",
    "        fake_output = self.discriminator(fake_images.detach())\n",
    "        fake_loss = self.adversarial_loss(fake_output, fake_labels)\n",
    "        \n",
    "        gradient_penalty = self.compute_gradient_penalty(real_images, fake_images)\n",
    "        \n",
    "        d_loss = (real_loss + fake_loss) / 2 + config.GRADIENT_PENALTY_LAMBDA * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer_D.step()\n",
    "        \n",
    "        return d_loss.item(), real_output.mean().item(), fake_output.mean().item()\n",
    "    \n",
    "    def compute_gradient_penalty(self, real_images, fake_images):\n",
    "        \"\"\"Compute gradient penalty.\"\"\"\n",
    "        batch_size = real_images.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, device=config.DEVICE)\n",
    "        alpha = alpha.expand_as(real_images)\n",
    "        \n",
    "        interpolated = alpha * real_images + (1 - alpha) * fake_images\n",
    "        interpolated.requires_grad_(True)\n",
    "        \n",
    "        d_interpolated = self.discriminator(interpolated)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        gradient_norm = gradients.view(batch_size, -1).norm(2, dim=1)\n",
    "        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "        \n",
    "        return gradient_penalty\n",
    "    \n",
    "    def train_generator(self, real_images):\n",
    "        \"\"\"Train generator.\"\"\"\n",
    "        self.optimizer_G.zero_grad()\n",
    "        \n",
    "        batch_size = real_images.size(0)\n",
    "        noise = torch.randn(batch_size, config.LATENT_DIM, device=config.DEVICE)\n",
    "        fake_images = self.generator(noise)\n",
    "        \n",
    "        fake_output = self.discriminator(fake_images)\n",
    "        real_labels = torch.ones(batch_size, device=config.DEVICE)\n",
    "        adversarial_loss = self.adversarial_loss(fake_output, real_labels)\n",
    "        \n",
    "        l1_loss = self.l1_loss(fake_images, real_images)\n",
    "        l2_loss = self.l2_loss(fake_images, real_images)\n",
    "        perceptual_loss = self.perceptual_loss(fake_images, real_images)\n",
    "        ssim_loss = self.ssim_loss(fake_images, real_images)\n",
    "        edge_loss = self.edge_loss(fake_images, real_images)\n",
    "        text_loss = self.text_structure_loss(fake_images, real_images)\n",
    "        \n",
    "        tv_loss = self.total_variation_loss(fake_images)\n",
    "        \n",
    "        content_loss = (\n",
    "            config.LAMBDA_L1 * l1_loss +\n",
    "            config.LAMBDA_L1 * 0.5 * l2_loss +\n",
    "            config.LAMBDA_PERCEPTUAL * perceptual_loss +\n",
    "            config.LAMBDA_SSIM * ssim_loss +\n",
    "            config.LAMBDA_EDGE * edge_loss +\n",
    "            config.LAMBDA_TEXT_STRUCTURE * text_loss +\n",
    "            config.LAMBDA_TV * tv_loss\n",
    "        )\n",
    "        \n",
    "        g_loss = config.LAMBDA_ADVERSARIAL * adversarial_loss + content_loss\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_np = fake_images.cpu().numpy()\n",
    "            real_np = real_images.cpu().numpy()\n",
    "            \n",
    "            ssim_scores = []\n",
    "            psnr_scores = []\n",
    "            \n",
    "            for i in range(min(4, batch_size)):\n",
    "                fake_img = ((fake_np[i, 0] + 1.0) / 2.0 * 255).astype(np.uint8)\n",
    "                real_img = ((real_np[i, 0] + 1.0) / 2.0 * 255).astype(np.uint8)\n",
    "                \n",
    "                try:\n",
    "                    ssim_score = ssim(real_img, fake_img, data_range=255)\n",
    "                    psnr_score = psnr(real_img, fake_img, data_range=255)\n",
    "                    ssim_scores.append(ssim_score)\n",
    "                    psnr_scores.append(psnr_score)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            avg_ssim = np.mean(ssim_scores) if ssim_scores else 0.0\n",
    "            avg_psnr = np.mean(psnr_scores) if psnr_scores else 0.0\n",
    "        \n",
    "        return {\n",
    "            'g_loss': g_loss.item(),\n",
    "            'adversarial_loss': adversarial_loss.item(),\n",
    "            'content_loss': content_loss.item(),\n",
    "            'ssim': avg_ssim,\n",
    "            'psnr': avg_psnr,\n",
    "            'fake_images': fake_images\n",
    "        }\n",
    "    \n",
    "    def total_variation_loss(self, images):\n",
    "        \"\"\"Total variation loss.\"\"\"\n",
    "        tv_h = torch.mean(torch.abs(images[:, :, 1:, :] - images[:, :, :-1, :]))\n",
    "        tv_w = torch.mean(torch.abs(images[:, :, :, 1:] - images[:, :, :, :-1]))\n",
    "        return tv_h + tv_w\n",
    "    \n",
    "    def save_training_progress(self, epoch, real_images, fake_images, num_samples=8):\n",
    "        \"\"\"Save training progress.\"\"\"\n",
    "        progress_folder = os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch:04d}\")\n",
    "        os.makedirs(progress_folder, exist_ok=True)\n",
    "        \n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            real_denorm = (real_images + 1.0) / 2.0\n",
    "            fake_denorm = (fake_images + 1.0) / 2.0\n",
    "            \n",
    "            fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 3, 6))\n",
    "            fig.suptitle(f'Training Progress - Epoch {epoch}', fontsize=16)\n",
    "            \n",
    "            for i in range(min(num_samples, real_images.size(0))):\n",
    "                real_img = real_denorm[i].cpu().squeeze().numpy()\n",
    "                axes[0, i].imshow(real_img, cmap='gray')\n",
    "                axes[0, i].set_title('Real', fontsize=10)\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                fake_img = fake_denorm[i].cpu().squeeze().numpy()\n",
    "                axes[1, i].imshow(fake_img, cmap='gray')\n",
    "                axes[1, i].set_title('Generated', fontsize=10)\n",
    "                axes[1, i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(progress_folder, 'comparison.png'), \n",
    "                       dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            for i in range(min(num_samples, fake_images.size(0))):\n",
    "                img_array = fake_denorm[i].cpu().squeeze().numpy()\n",
    "                img_array = (img_array * 255).astype(np.uint8)\n",
    "                \n",
    "                Image.fromarray(img_array, mode='L').save(\n",
    "                    os.path.join(progress_folder, f'generated_{i+1:02d}.png')\n",
    "                )\n",
    "        \n",
    "        self.generator.train()\n",
    "        logger.info(f\"Saved progress for epoch {epoch}\")\n",
    "    \n",
    "    def save_loss_plots(self):\n",
    "        \"\"\"Save loss plots.\"\"\"\n",
    "        if len(self.history['g_loss']) < 2:\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs = range(1, len(self.history['g_loss']) + 1)\n",
    "        \n",
    "        ax1.plot(epochs, self.history['g_loss'], 'b-', label='Generator', linewidth=2)\n",
    "        ax1.plot(epochs, self.history['d_loss'], 'r-', label='Discriminator', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Losses')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        if len(self.history['ssim_scores']) > 0:\n",
    "            ax2.plot(epochs, self.history['ssim_scores'], 'g-', label='SSIM', linewidth=2)\n",
    "            ax2.axhline(y=config.TARGET_SSIM, color='g', linestyle='--', alpha=0.7, label='Target')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('SSIM Score')\n",
    "            ax2.set_title('SSIM Quality')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        if len(self.history['psnr_scores']) > 0:\n",
    "            ax3.plot(epochs, self.history['psnr_scores'], 'm-', label='PSNR', linewidth=2)\n",
    "            ax3.axhline(y=config.TARGET_PSNR, color='m', linestyle='--', alpha=0.7, label='Target')\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('PSNR (dB)')\n",
    "            ax3.set_title('PSNR Quality')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        if len(self.history['g_adv_loss']) > 0:\n",
    "            ax4.plot(epochs, self.history['g_adv_loss'], 'c-', label='Adversarial', linewidth=2)\n",
    "            ax4.plot(epochs, self.history['g_content_loss'], 'y-', label='Content', linewidth=2)\n",
    "            ax4.set_xlabel('Epoch')\n",
    "            ax4.set_ylabel('Loss Component')\n",
    "            ax4.set_title('Generator Components')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.OUTPUT_DIR, 'training_metrics.png'), \n",
    "                   dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def train(self, dataloader, start_epoch=0):\n",
    "        \"\"\"Training loop.\"\"\"\n",
    "        logger.info(\"Starting training...\")\n",
    "        \n",
    "        best_ssim = 0.0\n",
    "        \n",
    "        for epoch in range(start_epoch, config.EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            d_losses = []\n",
    "            g_losses = []\n",
    "            g_adv_losses = []\n",
    "            g_content_losses = []\n",
    "            ssim_scores = []\n",
    "            psnr_scores = []\n",
    "            \n",
    "            real_scores = []\n",
    "            fake_scores = []\n",
    "            \n",
    "            for batch_idx, real_images in enumerate(dataloader):\n",
    "                real_images = real_images.to(config.DEVICE)\n",
    "                \n",
    "                d_loss, real_score, fake_score = self.train_discriminator(real_images)\n",
    "                d_losses.append(d_loss)\n",
    "                real_scores.append(real_score)\n",
    "                fake_scores.append(fake_score)\n",
    "                \n",
    "                g_results = self.train_generator(real_images)\n",
    "                g_losses.append(g_results['g_loss'])\n",
    "                g_adv_losses.append(g_results['adversarial_loss'])\n",
    "                g_content_losses.append(g_results['content_loss'])\n",
    "                ssim_scores.append(g_results['ssim'])\n",
    "                psnr_scores.append(g_results['psnr'])\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    logger.info(\n",
    "                        f'Epoch [{epoch+1:3d}/{config.EPOCHS}] '\n",
    "                        f'Batch [{batch_idx:3d}/{len(dataloader)}] '\n",
    "                        f'D_Loss: {d_loss:.4f} G_Loss: {g_results[\"g_loss\"]:.4f} '\n",
    "                        f'SSIM: {g_results[\"ssim\"]:.3f} PSNR: {g_results[\"psnr\"]:.2f}'\n",
    "                    )\n",
    "                \n",
    "                if (epoch == 0 and batch_idx % 50 == 0) or (epoch % 25 == 0 and batch_idx == 0):\n",
    "                    self.save_training_progress(epoch + 1, real_images, g_results['fake_images'])\n",
    "            \n",
    "            epoch_d_loss = np.mean(d_losses)\n",
    "            epoch_g_loss = np.mean(g_losses)\n",
    "            epoch_ssim = np.mean([s for s in ssim_scores if s > 0])\n",
    "            epoch_psnr = np.mean([p for p in psnr_scores if p > 0])\n",
    "            \n",
    "            self.history['d_loss'].append(epoch_d_loss)\n",
    "            self.history['g_loss'].append(epoch_g_loss)\n",
    "            self.history['g_adv_loss'].append(np.mean(g_adv_losses))\n",
    "            self.history['g_content_loss'].append(np.mean(g_content_losses))\n",
    "            self.history['ssim_scores'].append(epoch_ssim)\n",
    "            self.history['psnr_scores'].append(epoch_psnr)\n",
    "            \n",
    "            self.scheduler_G.step(epoch_g_loss)\n",
    "            self.scheduler_D.step(epoch_d_loss)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            logger.info(\n",
    "                f'Epoch [{epoch+1:3d}/{config.EPOCHS}] completed in {epoch_time:.2f}s - '\n",
    "                f'D_Loss: {epoch_d_loss:.4f} G_Loss: {epoch_g_loss:.4f} '\n",
    "                f'SSIM: {epoch_ssim:.3f} PSNR: {epoch_psnr:.2f}'\n",
    "            )\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_generated_images_only(epoch + 1)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.save_loss_plots()\n",
    "            \n",
    "            if epoch_ssim > best_ssim:\n",
    "                best_ssim = epoch_ssim\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'generator_state_dict': self.generator.state_dict(),\n",
    "                    'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "                    'optimizer_G_state_dict': self.optimizer_G.state_dict(),\n",
    "                    'optimizer_D_state_dict': self.optimizer_D.state_dict(),\n",
    "                    'history': self.history,\n",
    "                    'best_ssim': best_ssim\n",
    "                }, os.path.join(config.MODELS_DIR, 'best_model.pth'))\n",
    "                \n",
    "                logger.info(f\"New best SSIM: {best_ssim:.4f} - Model saved!\")\n",
    "            \n",
    "            if (epoch + 1) % 25 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'generator_state_dict': self.generator.state_dict(),\n",
    "                    'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "                    'history': self.history\n",
    "                }, os.path.join(config.MODELS_DIR, f'checkpoint_epoch_{epoch+1:04d}.pth'))\n",
    "        \n",
    "        logger.info(f\"Training completed! Best SSIM: {best_ssim:.4f}\")\n",
    "        self.save_loss_plots()\n",
    "    \n",
    "    def save_generated_images_only(self, epoch, num_samples=16):\n",
    "        \"\"\"Save generated images.\"\"\"\n",
    "        epoch_folder = os.path.join(config.GENERATED_IMAGES_DIR, f\"epoch_{epoch:04d}\")\n",
    "        os.makedirs(epoch_folder, exist_ok=True)\n",
    "        \n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(num_samples, config.LATENT_DIM, device=config.DEVICE)\n",
    "            fake_images = self.generator(noise)\n",
    "            \n",
    "            fake_denorm = (fake_images + 1.0) / 2.0\n",
    "            \n",
    "            for i in range(fake_images.size(0)):\n",
    "                img_array = fake_denorm[i].cpu().squeeze().numpy()\n",
    "                img_array = (img_array * 255).astype(np.uint8)\n",
    "                \n",
    "                # Sharpening filter\n",
    "                kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]])\n",
    "                img_array = cv2.filter2D(img_array, -1, kernel)\n",
    "                img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                Image.fromarray(img_array, 'L').save(\n",
    "                    os.path.join(epoch_folder, f'generated_{i+1:02d}.png')\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Saved {num_samples} images to {epoch_folder}\")\n",
    "        \n",
    "        self.generator.train()\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "def main():\n",
    "    \"\"\"Main function.\"\"\"\n",
    "    logger.info(\"Starting Enhanced Document GAN\")\n",
    "    logger.info(f\"Resolution: {config.IMG_WIDTH}x{config.IMG_HEIGHT}\")\n",
    "    logger.info(f\"Device: {config.DEVICE}\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    logger.info(\"Creating dataset...\")\n",
    "    dataset = ReadableDocumentDataset(config.REAL_IMAGES_DIR, create_samples=True)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=DATALOADER_NUM_WORKERS,\n",
    "        drop_last=True,\n",
    "        pin_memory=True if config.DEVICE.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Dataset: {len(dataset)} images, Batches: {len(dataloader)}\")\n",
    "    \n",
    "    try:\n",
    "        test_batch = next(iter(dataloader))\n",
    "        logger.info(f\"Batch shape: {test_batch.shape}\")\n",
    "        logger.info(f\"Value range: [{test_batch.min():.3f}, {test_batch.max():.3f}]\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error testing dataloader: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        trainer = EnhancedGANTrainer()\n",
    "        logger.info(\"Trainer initialized successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing trainer: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train(dataloader)\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        logger.info(f\"Models: {config.MODELS_DIR}\")\n",
    "        logger.info(f\"Images: {config.GENERATED_IMAGES_DIR}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Training interrupted\")\n",
    "        torch.save({\n",
    "            'generator_state_dict': trainer.generator.state_dict(),\n",
    "            'discriminator_state_dict': trainer.discriminator.state_dict(),\n",
    "            'history': trainer.history\n",
    "        }, os.path.join(config.MODELS_DIR, 'interrupted_checkpoint.pth'))\n",
    "        logger.info(\"Checkpoint saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        try:\n",
    "            torch.save({\n",
    "                'generator_state_dict': trainer.generator.state_dict(),\n",
    "                'discriminator_state_dict': trainer.discriminator.state_dict(),\n",
    "                'history': trainer.history\n",
    "            }, os.path.join(config.MODELS_DIR, 'error_checkpoint.pth'))\n",
    "            logger.info(\"Error checkpoint saved\")\n",
    "        except:\n",
    "            logger.error(\"Could not save checkpoint\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5723153-3ef1-490c-8107-17900158f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "class GANMetricsPlotter:\n",
    "    \"\"\"Plot comprehensive metrics for GAN training analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir=\"Enhanced_GAN_Models\", output_dir=\"Enhanced_GAN_Output\"):\n",
    "        self.models_dir = models_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.history = None\n",
    "        \n",
    "    def load_checkpoint(self, checkpoint_name='best_model.pth'):\n",
    "        \"\"\"Load training history from checkpoint.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.models_dir, checkpoint_name)\n",
    "        \n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "            print(\"Available checkpoints:\")\n",
    "            if os.path.exists(self.models_dir):\n",
    "                for file in os.listdir(self.models_dir):\n",
    "                    if file.endswith('.pth'):\n",
    "                        print(f\"  - {file}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            self.history = checkpoint.get('history', {})\n",
    "            \n",
    "            print(f\"Loaded checkpoint: {checkpoint_name}\")\n",
    "            print(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "            print(f\"Best SSIM: {checkpoint.get('best_ssim', 'N/A')}\")\n",
    "            print(f\"Available metrics: {list(self.history.keys())}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def plot_all_metrics(self, save_name='comprehensive_metrics.png'):\n",
    "        \"\"\"Create comprehensive visualization of all training metrics.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No history data loaded. Please load a checkpoint first.\")\n",
    "            return\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        epochs = range(1, len(self.history.get('g_loss', [])) + 1)\n",
    "        \n",
    "        # 1. Generator and Discriminator Loss\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        if 'g_loss' in self.history and 'd_loss' in self.history:\n",
    "            ax1.plot(epochs, self.history['g_loss'], 'b-', label='Generator Loss', linewidth=2, alpha=0.8)\n",
    "            ax1.plot(epochs, self.history['d_loss'], 'r-', label='Discriminator Loss', linewidth=2, alpha=0.8)\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training Losses', fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. SSIM Score with Target Line\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        if 'ssim_scores' in self.history and len(self.history['ssim_scores']) > 0:\n",
    "            ax2.plot(epochs, self.history['ssim_scores'], 'g-', label='SSIM Score', linewidth=2)\n",
    "            ax2.axhline(y=0.85, color='g', linestyle='--', alpha=0.7, label='Target SSIM (0.85)')\n",
    "            ax2.fill_between(epochs, self.history['ssim_scores'], alpha=0.3, color='g')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('SSIM Score')\n",
    "            ax2.set_title('Structural Similarity Index (SSIM)', fontweight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_ylim([0, 1])\n",
    "        \n",
    "        # 3. PSNR Score with Target Line\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        if 'psnr_scores' in self.history and len(self.history['psnr_scores']) > 0:\n",
    "            ax3.plot(epochs, self.history['psnr_scores'], 'm-', label='PSNR Score', linewidth=2)\n",
    "            ax3.axhline(y=30.0, color='m', linestyle='--', alpha=0.7, label='Target PSNR (30 dB)')\n",
    "            ax3.fill_between(epochs, self.history['psnr_scores'], alpha=0.3, color='m')\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('PSNR (dB)')\n",
    "            ax3.set_title('Peak Signal-to-Noise Ratio (PSNR)', fontweight='bold')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Generator Loss Components\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        if 'g_adv_loss' in self.history and 'g_content_loss' in self.history:\n",
    "            ax4.plot(epochs, self.history['g_adv_loss'], 'c-', label='Adversarial Loss', linewidth=2, alpha=0.8)\n",
    "            ax4.plot(epochs, self.history['g_content_loss'], 'y-', label='Content Loss', linewidth=2, alpha=0.8)\n",
    "            ax4.set_xlabel('Epoch')\n",
    "            ax4.set_ylabel('Loss Value')\n",
    "            ax4.set_title('Generator Loss Components', fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            ax4.set_yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # 5. Loss Ratio (G/D Balance)\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "        if 'g_loss' in self.history and 'd_loss' in self.history:\n",
    "            loss_ratio = np.array(self.history['g_loss']) / (np.array(self.history['d_loss']) + 1e-8)\n",
    "            ax5.plot(epochs, loss_ratio, 'purple', linewidth=2, alpha=0.8)\n",
    "            ax5.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7, label='Perfect Balance')\n",
    "            ax5.set_xlabel('Epoch')\n",
    "            ax5.set_ylabel('G_Loss / D_Loss Ratio')\n",
    "            ax5.set_title('Training Balance (Generator/Discriminator)', fontweight='bold')\n",
    "            ax5.legend()\n",
    "            ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Quality Improvement Over Time\n",
    "        ax6 = fig.add_subplot(gs[1, 2])\n",
    "        if 'ssim_scores' in self.history and 'psnr_scores' in self.history:\n",
    "            # Normalize both metrics to 0-1 range for comparison\n",
    "            ssim_norm = np.array(self.history['ssim_scores'])\n",
    "            psnr_norm = np.array(self.history['psnr_scores']) / 50.0  # Normalize PSNR\n",
    "            \n",
    "            ax6.plot(epochs, ssim_norm, 'g-', label='SSIM (normalized)', linewidth=2, alpha=0.8)\n",
    "            ax6.plot(epochs, psnr_norm, 'm-', label='PSNR (normalized)', linewidth=2, alpha=0.8)\n",
    "            ax6.set_xlabel('Epoch')\n",
    "            ax6.set_ylabel('Normalized Score')\n",
    "            ax6.set_title('Quality Metrics Comparison', fontweight='bold')\n",
    "            ax6.legend()\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 7. Loss Convergence Analysis\n",
    "        ax7 = fig.add_subplot(gs[2, 0])\n",
    "        if 'g_loss' in self.history:\n",
    "            # Moving average for trend analysis\n",
    "            window = min(10, len(self.history['g_loss']) // 10)\n",
    "            if window > 1:\n",
    "                g_loss_ma = np.convolve(self.history['g_loss'], \n",
    "                                       np.ones(window)/window, mode='valid')\n",
    "                d_loss_ma = np.convolve(self.history['d_loss'], \n",
    "                                       np.ones(window)/window, mode='valid')\n",
    "                \n",
    "                ax7.plot(range(1, len(g_loss_ma) + 1), g_loss_ma, \n",
    "                        'b-', label=f'Generator (MA-{window})', linewidth=2)\n",
    "                ax7.plot(range(1, len(d_loss_ma) + 1), d_loss_ma, \n",
    "                        'r-', label=f'Discriminator (MA-{window})', linewidth=2)\n",
    "                ax7.set_xlabel('Epoch')\n",
    "                ax7.set_ylabel('Loss (Moving Average)')\n",
    "                ax7.set_title('Loss Convergence Trend', fontweight='bold')\n",
    "                ax7.legend()\n",
    "                ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 8. Quality Score Distribution\n",
    "        ax8 = fig.add_subplot(gs[2, 1])\n",
    "        if 'ssim_scores' in self.history and len(self.history['ssim_scores']) > 0:\n",
    "            ax8.hist(self.history['ssim_scores'], bins=30, alpha=0.7, color='g', edgecolor='black')\n",
    "            ax8.axvline(np.mean(self.history['ssim_scores']), color='darkgreen', \n",
    "                       linestyle='--', linewidth=2, label=f'Mean: {np.mean(self.history[\"ssim_scores\"]):.3f}')\n",
    "            ax8.set_xlabel('SSIM Score')\n",
    "            ax8.set_ylabel('Frequency')\n",
    "            ax8.set_title('SSIM Score Distribution', fontweight='bold')\n",
    "            ax8.legend()\n",
    "            ax8.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 9. Training Summary Statistics\n",
    "        ax9 = fig.add_subplot(gs[2, 2])\n",
    "        ax9.axis('off')\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary_text = \"Training Summary Statistics\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "        \n",
    "        if 'g_loss' in self.history:\n",
    "            summary_text += f\"Generator Loss:\\n\"\n",
    "            summary_text += f\"  Final: {self.history['g_loss'][-1]:.4f}\\n\"\n",
    "            summary_text += f\"  Mean: {np.mean(self.history['g_loss']):.4f}\\n\"\n",
    "            summary_text += f\"  Min: {np.min(self.history['g_loss']):.4f}\\n\\n\"\n",
    "        \n",
    "        if 'd_loss' in self.history:\n",
    "            summary_text += f\"Discriminator Loss:\\n\"\n",
    "            summary_text += f\"  Final: {self.history['d_loss'][-1]:.4f}\\n\"\n",
    "            summary_text += f\"  Mean: {np.mean(self.history['d_loss']):.4f}\\n\"\n",
    "            summary_text += f\"  Min: {np.min(self.history['d_loss']):.4f}\\n\\n\"\n",
    "        \n",
    "        if 'ssim_scores' in self.history and len(self.history['ssim_scores']) > 0:\n",
    "            summary_text += f\"SSIM Score:\\n\"\n",
    "            summary_text += f\"  Final: {self.history['ssim_scores'][-1]:.4f}\\n\"\n",
    "            summary_text += f\"  Mean: {np.mean(self.history['ssim_scores']):.4f}\\n\"\n",
    "            summary_text += f\"  Best: {np.max(self.history['ssim_scores']):.4f}\\n\\n\"\n",
    "        \n",
    "        if 'psnr_scores' in self.history and len(self.history['psnr_scores']) > 0:\n",
    "            summary_text += f\"PSNR Score:\\n\"\n",
    "            summary_text += f\"  Final: {self.history['psnr_scores'][-1]:.2f} dB\\n\"\n",
    "            summary_text += f\"  Mean: {np.mean(self.history['psnr_scores']):.2f} dB\\n\"\n",
    "            summary_text += f\"  Best: {np.max(self.history['psnr_scores']):.2f} dB\\n\"\n",
    "        \n",
    "        ax9.text(0.1, 0.9, summary_text, transform=ax9.transAxes, \n",
    "                fontfamily='monospace', fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        # Overall title\n",
    "        fig.suptitle('Enhanced Document GAN - Training Metrics Analysis', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Save figure\n",
    "        save_path = os.path.join(self.output_dir, save_name)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Comprehensive metrics plot saved to: {save_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_accuracy_analysis(self, save_name='accuracy_analysis.png'):\n",
    "        \"\"\"Plot detailed accuracy/quality analysis.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No history data loaded.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        epochs = range(1, len(self.history.get('ssim_scores', [])) + 1)\n",
    "        \n",
    "        # 1. Quality Score Evolution\n",
    "        ax1 = axes[0, 0]\n",
    "        if 'ssim_scores' in self.history:\n",
    "            ax1.plot(epochs, self.history['ssim_scores'], 'g-', linewidth=2, label='SSIM')\n",
    "            # Add trend line\n",
    "            z = np.polyfit(range(len(self.history['ssim_scores'])), \n",
    "                          self.history['ssim_scores'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax1.plot(epochs, p(range(len(self.history['ssim_scores']))), \n",
    "                    \"g--\", alpha=0.5, label='Trend')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('SSIM Score')\n",
    "            ax1.set_title('SSIM Accuracy Evolution', fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. PSNR Evolution\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'psnr_scores' in self.history:\n",
    "            ax2.plot(epochs, self.history['psnr_scores'], 'm-', linewidth=2, label='PSNR')\n",
    "            z = np.polyfit(range(len(self.history['psnr_scores'])), \n",
    "                          self.history['psnr_scores'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax2.plot(epochs, p(range(len(self.history['psnr_scores']))), \n",
    "                    \"m--\", alpha=0.5, label='Trend')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('PSNR (dB)')\n",
    "            ax2.set_title('PSNR Accuracy Evolution', fontweight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Combined Quality Score\n",
    "        ax3 = axes[1, 0]\n",
    "        if 'ssim_scores' in self.history and 'psnr_scores' in self.history:\n",
    "            # Normalize and combine\n",
    "            ssim_norm = (np.array(self.history['ssim_scores']) - 0.5) / 0.5\n",
    "            psnr_norm = (np.array(self.history['psnr_scores']) - 20) / 30\n",
    "            combined = (ssim_norm + psnr_norm) / 2\n",
    "            \n",
    "            ax3.plot(epochs, combined, 'b-', linewidth=2, label='Combined Quality Score')\n",
    "            ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Good Quality Threshold')\n",
    "            ax3.fill_between(epochs, combined, alpha=0.3)\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Normalized Combined Score')\n",
    "            ax3.set_title('Overall Quality Score (SSIM + PSNR)', fontweight='bold')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Improvement Rate\n",
    "        ax4 = axes[1, 1]\n",
    "        if 'ssim_scores' in self.history and len(self.history['ssim_scores']) > 1:\n",
    "            improvement = np.diff(self.history['ssim_scores'])\n",
    "            ax4.plot(range(2, len(epochs) + 1), improvement, 'orange', linewidth=2)\n",
    "            ax4.axhline(y=0, color='red', linestyle='-', alpha=0.5)\n",
    "            ax4.set_xlabel('Epoch')\n",
    "            ax4.set_ylabel('SSIM Improvement')\n",
    "            ax4.set_title('Quality Improvement Rate per Epoch', fontweight='bold')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(self.output_dir, save_name)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Accuracy analysis plot saved to: {save_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def print_detailed_statistics(self):\n",
    "        \"\"\"Print detailed training statistics.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No history data loaded.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED TRAINING STATISTICS\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        metrics = {\n",
    "            'Generator Loss': 'g_loss',\n",
    "            'Discriminator Loss': 'd_loss',\n",
    "            'SSIM Score': 'ssim_scores',\n",
    "            'PSNR Score': 'psnr_scores',\n",
    "            'Adversarial Loss': 'g_adv_loss',\n",
    "            'Content Loss': 'g_content_loss'\n",
    "        }\n",
    "        \n",
    "        for name, key in metrics.items():\n",
    "            if key in self.history and len(self.history[key]) > 0:\n",
    "                values = np.array(self.history[key])\n",
    "                print(f\"{name}:\")\n",
    "                print(f\"  Initial:  {values[0]:.4f}\")\n",
    "                print(f\"  Final:    {values[-1]:.4f}\")\n",
    "                print(f\"  Best:     {np.max(values) if 'score' in key.lower() else np.min(values):.4f}\")\n",
    "                print(f\"  Mean:     {np.mean(values):.4f}\")\n",
    "                print(f\"  Std Dev:  {np.std(values):.4f}\")\n",
    "                \n",
    "                # Calculate improvement\n",
    "                if len(values) > 1:\n",
    "                    if 'score' in key.lower():\n",
    "                        improvement = ((values[-1] - values[0]) / values[0]) * 100\n",
    "                    else:\n",
    "                        improvement = ((values[0] - values[-1]) / values[0]) * 100\n",
    "                    print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "                print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to generate all plots.\"\"\"\n",
    "    print(\"GAN Training Metrics Plotter\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize plotter\n",
    "    plotter = GANMetricsPlotter()\n",
    "    \n",
    "    # Try to load the best model\n",
    "    if plotter.load_checkpoint('best_model.pth'):\n",
    "        print(\"\\nGenerating plots...\")\n",
    "        \n",
    "        # Generate comprehensive metrics plot\n",
    "        plotter.plot_all_metrics('comprehensive_metrics.png')\n",
    "        \n",
    "        # Generate accuracy analysis\n",
    "        plotter.plot_accuracy_analysis('accuracy_analysis.png')\n",
    "        \n",
    "        # Print detailed statistics\n",
    "        plotter.print_detailed_statistics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"All plots generated successfully!\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\nFailed to load checkpoint. Please ensure training has been completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
